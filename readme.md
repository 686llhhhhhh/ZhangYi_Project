# Introduction

The software is based on the optimal GBR model and allows the user to input the corresponding biochar property data and anaerobic digestion data to obtain the methane yield (MY) and maximum methane production rate (Rmax) predicted by the software. 

The researcher can also obtain the model predictions for MY and R2 for Rmax for comparative evaluation. 

Based on the software, optimal biochar properties can be obtained for a fixed anaerobic digestion condition to better guide practical experiments.

- GBR
The GBR model is a technology that learns from its mistakes. In essence, it is brainstorming and integrating a bunch of poor learning algorithms for learning. Using the boosting strategy for training, boosting is a general regression analysis algorithm to enhance the performance of the basic algorithm. It can improve the weak learning algorithm into a strong one and can be applied to other basic regression algorithms to improve the accuracy. Boosting is an iterative learning process. Specifically, a basic learner is trained from the initial training set, and then the distribution of training samples is adjusted according to the performance of the basic learner so that the samples improperly processed by the previous basic learner will receive more attention in the subsequent training process; Then the next base learner is trained based on the adjusted sample distribution; This is repeated until the number of base learners reaches the predetermined value T, or more base learners cannot reduce the residual, and then this base learner is weighted and combined. In this work, the number of boost stages, the maximum depth of the base learner, the loss function, the learning rate, and the minimum sample are adjusted to improve the prediction accuracy.

- RF
The random forest model is an integrated learning method of the machine learning model (nonlinear tree-based model). Randomize the use of variables (columns) and data (rows), generate many classification trees, and then summarize the classification tree results. Random forest improves the prediction accuracy without significantly increasing the amount of computation. Random forest is insensitive to multicollinearity, and the result is relatively robust to missing and unbalanced data, which can well predict the role of up to thousands of explanatory variables. Random forest randomly builds a forest. The forest is composed of many decision trees, and there is no correlation between each decision tree. After obtaining the random forest model, when new samples enter, each decision tree in the random forest makes judgments, respectively. The bagging set strategy is relatively simple. The voting method is usually used for classification problems, and the category with the most votes or one of the categories is the final model output. The simple average method is usually used for regression, and the regression results obtained by T weak learners are arithmetically averaged, that is, the final model output.

- XGBoost
XGBoost model is an optimized distributed gradient lifting library designed to be efficient, flexible, and portable. The basic idea of XGBoost can almost be understood as taking the second-order Taylor expansion of the loss function as its replacement function and solving its minimization (derivative is 0) to determine the best segmentation point and leaf node output value of the regression tree (this differs from cart regression tree). In addition, XGBoost fully considers the regularization problem by introducing the number of subtrees and the number of subtree nodes into the loss function, which can effectively avoid overfitting. In terms of efficiency, XGBoost has greatly improved the modeling efficiency compared with the general GBDT by using the unique methods of approximate regression tree bifurcation point estimation and sub-node parallelization, plus the characteristics of second-order convergence. Its basic principle is to generate a tree from the objective function, and then after t iterations, the final prediction result is received. The prediction result of the previous T-1 tree is related to the model of the *t*th tree. The complexity of the decision tree can be composed of the number of leaves. The fewer leaf nodes, the simpler the model is. In addition, leaf nodes should not contain too much weight (similar to the weight of each variable of LR). Therefore, the regular term of the objective function is determined by the number of leaf nodes of all decision trees generated and the normal form of the vector composed of the weight of all nodes.

## Development Environment

Python (3.8) Pyside2

## Google Cloud Drive Sharing

https://drive.google.com/file/d/1_GSrpuaAtpoeML5iOxG4UhZDsynDGfqz/view?usp=sharing